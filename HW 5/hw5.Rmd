---
title: "HW 5 Q2"
author: "Ben Howell"
date: "3/24/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      fig.align = TRUE,
                      fig.width = 6,
                      fig.height = 4)
```

```{r}
require(tidyverse)
require(janitor)
require(purrr)
require(leaps)
require(ggthemes)

# generate p = 20, n = 1000

set.seed(123)

df <- data.frame(replicate(20, rnorm(n = 1000, mean = 15, sd = 3)))
df <- df %>%
  clean_names()

df <- df %>%
  dplyr::mutate(y = ifelse(rowSums(.) < 320, 
                           rnorm(1, mean = 5, sd = 1), 0) * rowSums(.) +
                  rnorm(1000, mean = 100, sd = 10))
  # if the rowSum is low, that total gets multiplied by a random number drawn from a distribution with a mean = 5
  # and a standard deviation of 1, otherwise it gets multiplied by 0
  # each observation then gets the error added to it, which is a number from the distribution where mean = 100/sd = 10
```

```{r}
train <- sample(nrow(df), 100)
x_train <- df[train,]
y_train <- df[train, 21]

x_test <- df[-train,]
y_test <- df[-train, 21]
```

```{r}
model <- regsubsets(y ~ ., data = x_test, nvmax = 20)
```

```{r}
sum <- summary(model)
names(sum)

sum$rsq

train_mat <- model.matrix(y ~ ., data = x_train)
errors <- rep(NA, 20)

for (i in 1:20) {
  
  cf <- coef(model, id = i)
  pred <- train_mat[, names(cf)]%*%cf
  errors[i] <- mean((y_train - pred)^2)
  
  # print(i)
}
```

```{r}
errors %>% 
  data.frame() %>% 
  clean_names() %>%
  mutate(predictors = row_number()) %>%
  ggplot() +
  geom_point(aes(x = predictors, y = x)) +
  geom_line(aes(x = predictors, y = x)) +
  labs(title = "MSE by # of Predictors on Training Set")
```

```{r}
test_nat <- model.matrix(y ~ ., data = x_test)
test_errors <- rep(NA, 20)

for (i in 1:20) {
  
  cf <- coef(model, id = i)
  pred <- test_nat[, names(cf)]%*%cf
  test_errors[i] <- mean((y_test - pred)^2)
  
  # print(i)
}
```

```{r}
test_errors %>% 
  data.frame() %>% 
  clean_names() %>%
  mutate(predictors = row_number()) %>%
  ggplot() +
  geom_point(aes(x = predictors, y = x)) +
  geom_line(aes(x = predictors, y = x)) +
  labs(title = "MSE by # of Predictors on Test Set")
```

```{r}
n <- which.min(test_errors)
print(paste0("The test set's MSE is minimized at n = ", n))
```

It's interesting to see that the test set's MSE was minimized at lots of predictors even at times being minimized at the highest number of predictors. It's interesting because the training set was minimized at n = 18, rather than n = 20. However, due to the extreme randomness of the simulated data, I believe that explains most of the contradiction there, and some of the weird nature of these results.                       

```{r}
coef(model, id = n)
```

```{r}
hist(df$y)
```




